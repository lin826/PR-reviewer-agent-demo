diff --git a/sklearn/feature_selection/_sequential.py b/sklearn/feature_selection/_sequential.py
index e983c55de..a15929d38 100644
--- a/sklearn/feature_selection/_sequential.py
+++ b/sklearn/feature_selection/_sequential.py
@@ -12,10 +12,11 @@ from ..base import BaseEstimator, MetaEstimatorMixin, clone
 from ..utils._param_validation import HasMethods, Hidden, Interval, StrOptions
 from ..utils._param_validation import RealNotInt
 from ..utils._tags import _safe_tags
 from ..utils.validation import check_is_fitted
 from ..model_selection import cross_val_score
+from ..model_selection._split import check_cv
 from ..metrics import get_scorer_names
 
 
 class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator):
     """Transformer that performs Sequential Feature Selection.
@@ -227,10 +228,16 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
             ensure_min_features=2,
             force_all_finite=not tags.get("allow_nan", True),
         )
         n_features = X.shape[1]
 
+        # Preprocess cv parameter to handle iterables properly
+        # This ensures that if cv is an iterable of splits, it gets converted
+        # to a reusable form that won't be exhausted after the first use
+        from ..base import is_classifier
+        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))
+
         # FIXME: to be fixed in 1.3
         error_msg = (
             "n_features_to_select must be either 'auto', 'warn', "
             "None, an integer in [1, n_features - 1] "
             "representing the absolute "
@@ -273,11 +280,11 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
 
         old_score = -np.inf
         is_auto_select = self.tol is not None and self.n_features_to_select == "auto"
         for _ in range(n_iterations):
             new_feature_idx, new_score = self._get_best_new_feature_score(
-                cloned_estimator, X, y, current_mask
+                cloned_estimator, X, y, current_mask, cv
             )
             if is_auto_select and ((new_score - old_score) < self.tol):
                 break
 
             old_score = new_score
@@ -289,11 +296,11 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
         self.support_ = current_mask
         self.n_features_to_select_ = self.support_.sum()
 
         return self
 
-    def _get_best_new_feature_score(self, estimator, X, y, current_mask):
+    def _get_best_new_feature_score(self, estimator, X, y, current_mask, cv):
         # Return the best new feature and its score to add to the current_mask,
         # i.e. return the best new feature and its score to add (resp. remove)
         # when doing forward selection (resp. backward selection).
         # Feature will be added if the current score and past score are greater
         # than tol when n_feature is auto,
@@ -307,11 +314,11 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
             X_new = X[:, candidate_mask]
             scores[feature_idx] = cross_val_score(
                 estimator,
                 X_new,
                 y,
-                cv=self.cv,
+                cv=cv,
                 scoring=self.scoring,
                 n_jobs=self.n_jobs,
             ).mean()
         new_feature_idx = max(scores, key=lambda feature_idx: scores[feature_idx])
         return new_feature_idx, scores[new_feature_idx]
