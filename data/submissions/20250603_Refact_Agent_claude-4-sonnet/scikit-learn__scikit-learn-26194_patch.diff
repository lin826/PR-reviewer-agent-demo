diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py
index 0cdead923..4c11db9dc 100644
--- a/sklearn/metrics/_ranking.py
+++ b/sklearn/metrics/_ranking.py
@@ -1083,7 +1083,12 @@ def roc_curve(
     # to make sure that the curve starts at (0, 0)
     tps = np.r_[0, tps]
     fps = np.r_[0, fps]
-    thresholds = np.r_[thresholds[0] + 1, thresholds]
+    # For probability estimates, ensure threshold doesn't exceed reasonable bounds
+    # Use a small epsilon above the max threshold, capped at 1.0 + epsilon
+    extra_threshold = thresholds[0] + 1e-8
+    if thresholds[0] >= 1.0:
+        extra_threshold = 1.0 + 1e-8
+    thresholds = np.r_[extra_threshold, thresholds]
 
     if fps[-1] <= 0:
         warnings.warn(
diff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py
index f38f118c3..6f03b8cec 100644
--- a/sklearn/metrics/tests/test_ranking.py
+++ b/sklearn/metrics/tests/test_ranking.py
@@ -418,13 +418,21 @@ def test_roc_curve_drop_intermediate():
     y_true = [0, 0, 0, 0, 1, 1]
     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]
     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)
-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])
+    # The first threshold should be slightly above max(y_score) but not exceed 1.0 + epsilon
+    # when max(y_score) = 1.0
+    expected_first_threshold = 1.0 + 1e-8
+    expected_thresholds = [expected_first_threshold, 1.0, 0.7, 0.0]
+    assert_array_almost_equal(thresholds, expected_thresholds)
 
     # Test dropping thresholds with repeating scores
     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]
     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)
-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])
+    # The first threshold should be slightly above max(y_score) but not exceed 1.0 + epsilon
+    # when max(y_score) = 1.0
+    expected_first_threshold = 1.0 + 1e-8
+    expected_thresholds = [expected_first_threshold, 1.0, 0.9, 0.7, 0.6, 0.0]
+    assert_array_almost_equal(thresholds, expected_thresholds)
 
 
 def test_roc_curve_fpr_tpr_increasing():
@@ -439,6 +447,39 @@ def test_roc_curve_fpr_tpr_increasing():
     assert (np.diff(tpr) < 0).sum() == 0
 
 
+def test_roc_curve_with_probability_estimates():
+    # Test that thresholds don't exceed 1 when providing probability estimates
+    # Regression test for issue where thresholds could exceed 1.0
+    rng = np.random.RandomState(42)
+    y_true = rng.randint(0, 2, size=10)
+    y_score = rng.rand(10)  # Probability estimates in [0, 1]
+    _, _, thresholds = roc_curve(y_true, y_score)
+    
+    # All thresholds should be in reasonable bounds for probability estimates
+    # Allow small epsilon above 1.0 for numerical stability
+    assert np.all(thresholds >= 0), "Thresholds should be non-negative"
+    assert np.all(thresholds <= 1.0 + 1e-7), "Thresholds should not significantly exceed 1.0"
+    
+    # Test edge cases
+    # Case 1: All scores are exactly 1.0
+    y_true = np.array([0, 1, 0, 1])
+    y_score = np.array([1.0, 1.0, 1.0, 1.0])
+    _, _, thresholds = roc_curve(y_true, y_score)
+    assert np.all(thresholds <= 1.0 + 1e-7), "Thresholds should not significantly exceed 1.0"
+    
+    # Case 2: Mix of 0 and 1 probabilities
+    y_true = np.array([0, 1, 0, 1])
+    y_score = np.array([0.0, 1.0, 0.0, 1.0])
+    _, _, thresholds = roc_curve(y_true, y_score)
+    assert np.all(thresholds <= 1.0 + 1e-7), "Thresholds should not significantly exceed 1.0"
+    
+    # Case 3: Probabilities very close to 1
+    y_true = np.array([0, 1, 0, 1])
+    y_score = np.array([0.99, 0.999, 0.9999, 1.0])
+    _, _, thresholds = roc_curve(y_true, y_score)
+    assert np.all(thresholds <= 1.0 + 1e-7), "Thresholds should not significantly exceed 1.0"
+
+
 def test_auc():
     # Test Area Under Curve (AUC) computation
     x = [0, 1]
