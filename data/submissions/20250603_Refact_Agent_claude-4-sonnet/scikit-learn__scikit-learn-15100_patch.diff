diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py
index 7b7697ff4..93250d43e 100644
--- a/sklearn/feature_extraction/tests/test_text.py
+++ b/sklearn/feature_extraction/tests/test_text.py
@@ -97,6 +97,19 @@ def test_strip_accents():
     expected = 'this is a test'
     assert strip_accents_unicode(a) == expected
 
+    # Test for strings already in NFKD form (issue with combining characters)
+    # Single codepoint Ã± vs decomposed n + combining tilde should both become 'n'
+    single_codepoint = chr(241)  # LATIN SMALL LETTER N WITH TILDE
+    decomposed = chr(110) + chr(771)  # LATIN SMALL LETTER N + COMBINING TILDE
+    expected = 'n'
+    assert strip_accents_unicode(single_codepoint) == expected
+    assert strip_accents_unicode(decomposed) == expected
+    
+    # Test with multiple combining characters
+    o_with_accents = chr(111) + chr(768) + chr(769)  # o + grave + acute
+    expected = 'o'
+    assert strip_accents_unicode(o_with_accents) == expected
+
 
 def test_to_ascii():
     # check some classical latin accentuated symbols
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index bb5a9d646..11236d958 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -130,10 +130,7 @@ def strip_accents_unicode(s):
         ASCII equivalent.
     """
     normalized = unicodedata.normalize('NFKD', s)
-    if normalized == s:
-        return s
-    else:
-        return ''.join([c for c in normalized if not unicodedata.combining(c)])
+    return ''.join([c for c in normalized if not unicodedata.combining(c)])
 
 
 def strip_accents_ascii(s):
