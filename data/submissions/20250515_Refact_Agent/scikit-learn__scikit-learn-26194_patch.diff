diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py
index 0cdead923..4cbec8189 100644
--- a/sklearn/metrics/_ranking.py
+++ b/sklearn/metrics/_ranking.py
@@ -1021,6 +1021,12 @@ def roc_curve(
         fpr and tpr. `thresholds[0]` represents no instances being predicted
         and is arbitrarily set to `max(y_score) + 1`.
 
+        Note that when providing probability estimates (scores in the range
+        [0, 1]), the first threshold will be greater than 1 since it equals
+        `max(y_score) + 1`. This is intentional and ensures that the ROC curve
+        starts at the point (0, 0), corresponding to a classifier that predicts
+        all samples as negative.
+
     See Also
     --------
     RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
diff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py
index f38f118c3..57c59af94 100644
--- a/sklearn/metrics/tests/test_ranking.py
+++ b/sklearn/metrics/tests/test_ranking.py
@@ -1915,6 +1915,26 @@ def _test_ndcg_score_for(y_true, y_score):
     assert score.shape == (y_true.shape[0],)
 
 
+def test_roc_curve_sentinel_threshold():
+    # Test that the first threshold in roc_curve is max(y_score) + 1
+    # and all other thresholds are <= max(y_score)
+    rng = np.random.RandomState(42)
+    y_true = rng.randint(0, 2, size=10)
+    y_score = rng.rand(10)  # Probabilities between 0 and 1
+    
+    _, _, thresholds = roc_curve(y_true, y_score)
+    
+    # Check that the first threshold is max(y_score) + 1
+    assert thresholds[0] == pytest.approx(y_score.max() + 1)
+    
+    # Check that all other thresholds are <= max(y_score)
+    assert np.all(thresholds[1:] <= y_score.max())
+    
+    # For probability estimates, all thresholds except the first should be in [0, 1]
+    assert np.all(thresholds[1:] >= 0)
+    assert np.all(thresholds[1:] <= 1)
+
+
 def test_partial_roc_auc_score():
     # Check `roc_auc_score` for max_fpr != `None`
     y_true = np.array([0, 0, 1, 1])
