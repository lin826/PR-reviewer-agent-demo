diff --git a/sklearn/feature_selection/_sequential.py b/sklearn/feature_selection/_sequential.py
index e983c55de..2d1c591d0 100644
--- a/sklearn/feature_selection/_sequential.py
+++ b/sklearn/feature_selection/_sequential.py
@@ -8,12 +8,12 @@ import numpy as np
 import warnings
 
 from ._base import SelectorMixin
-from ..base import BaseEstimator, MetaEstimatorMixin, clone
+from ..base import BaseEstimator, MetaEstimatorMixin, clone, is_classifier
 from ..utils._param_validation import HasMethods, Hidden, Interval, StrOptions
 from ..utils._param_validation import RealNotInt
 from ..utils._tags import _safe_tags
 from ..utils.validation import check_is_fitted
-from ..model_selection import cross_val_score
+from ..model_selection import cross_val_score, check_cv
 from ..metrics import get_scorer_names
 
 
@@ -260,6 +260,9 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
             raise ValueError("tol must be positive when doing forward selection")
 
         cloned_estimator = clone(self.estimator)
+        
+        # Convert cv to a proper CV splitter and store it
+        self._cv = check_cv(self.cv, y, classifier=is_classifier(cloned_estimator))
 
         # the current mask corresponds to the set of features:
         # - that we have already *selected* if we do forward selection
@@ -305,11 +308,15 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
             if self.direction == "backward":
                 candidate_mask = ~candidate_mask
             X_new = X[:, candidate_mask]
+            
+            # Create fresh CV splits for each feature evaluation
+            cv_splits = list(self._cv.split(X, y))
+            
             scores[feature_idx] = cross_val_score(
                 estimator,
                 X_new,
                 y,
-                cv=self.cv,
+                cv=cv_splits,
                 scoring=self.scoring,
                 n_jobs=self.n_jobs,
             ).mean()

