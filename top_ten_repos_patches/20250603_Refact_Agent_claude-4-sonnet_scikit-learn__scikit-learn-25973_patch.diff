diff --git a/sklearn/feature_selection/_sequential.py b/sklearn/feature_selection/_sequential.py
index e983c55de..b061323b5 100644
--- a/sklearn/feature_selection/_sequential.py
+++ b/sklearn/feature_selection/_sequential.py
@@ -14,6 +14,8 @@ from ..utils._param_validation import RealNotInt
 from ..utils._tags import _safe_tags
 from ..utils.validation import check_is_fitted
 from ..model_selection import cross_val_score
+from ..model_selection._validation import check_cv
+from ..base import is_classifier
 from ..metrics import get_scorer_names
 
 
@@ -186,7 +188,7 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
         self.cv = cv
         self.n_jobs = n_jobs
 
-    def fit(self, X, y=None):
+    def fit(self, X, y=None, groups=None):
         """Learn the features to select from X.
 
         Parameters
@@ -199,6 +201,11 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
             Target values. This parameter may be ignored for
             unsupervised learning.
 
+        groups : array-like of shape (n_samples,), default=None
+            Group labels for the samples used while splitting the dataset into
+            train/test set. Only used in conjunction with a "Group" :term:`cv`
+            instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).
+
         Returns
         -------
         self : object
@@ -206,6 +213,9 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
         """
         self._validate_params()
 
+        # Normalize cv parameter to handle generators properly
+        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))
+
         # FIXME: to be removed in 1.3
         if self.n_features_to_select in ("warn", None):
             # for backwards compatibility
@@ -275,7 +285,7 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
         is_auto_select = self.tol is not None and self.n_features_to_select == "auto"
         for _ in range(n_iterations):
             new_feature_idx, new_score = self._get_best_new_feature_score(
-                cloned_estimator, X, y, current_mask
+                cloned_estimator, X, y, current_mask, cv, groups
             )
             if is_auto_select and ((new_score - old_score) < self.tol):
                 break
@@ -291,7 +301,7 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
 
         return self
 
-    def _get_best_new_feature_score(self, estimator, X, y, current_mask):
+    def _get_best_new_feature_score(self, estimator, X, y, current_mask, cv, groups):
         # Return the best new feature and its score to add to the current_mask,
         # i.e. return the best new feature and its score to add (resp. remove)
         # when doing forward selection (resp. backward selection).
@@ -309,7 +319,8 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator
                 estimator,
                 X_new,
                 y,
-                cv=self.cv,
+                groups=groups,
+                cv=cv,
                 scoring=self.scoring,
                 n_jobs=self.n_jobs,
             ).mean()
