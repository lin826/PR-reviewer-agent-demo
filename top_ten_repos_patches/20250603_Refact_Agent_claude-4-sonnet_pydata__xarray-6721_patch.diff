diff --git a/xarray/core/common.py b/xarray/core/common.py
index 3c328f42..a3636cba 100644
--- a/xarray/core/common.py
+++ b/xarray/core/common.py
@@ -2023,7 +2023,7 @@ def get_chunksizes(
 
     chunks: dict[Any, tuple[int, ...]] = {}
     for v in variables:
-        if hasattr(v.data, "chunks"):
+        if hasattr(v._data, "chunks"):
             for dim, c in v.chunksizes.items():
                 if dim in chunks and c != chunks[dim]:
                     raise ValueError(
diff --git a/xarray/tests/test_dataset.py b/xarray/tests/test_dataset.py
index e3ab430a..492f231c 100644
--- a/xarray/tests/test_dataset.py
+++ b/xarray/tests/test_dataset.py
@@ -1049,6 +1049,65 @@ class TestDataset:
         with pytest.raises(ValueError, match=r"some chunks"):
             data.chunk({"foo": 10})
 
+    def test_chunks_does_not_load_data(self) -> None:
+        """Test that accessing Dataset.chunks doesn't trigger data loading for zarr-like arrays."""
+        # Create a numpy array that we'll wrap to simulate zarr backend behavior
+        base_array = np.random.random((100, 100))
+        
+        # Wrap it in adapters to simulate zarr backend
+        numpy_adapter = indexing.NumpyIndexingAdapter(base_array)
+        lazy_array = indexing.LazilyIndexedArray(numpy_adapter)
+        cached_array = indexing.MemoryCachedArray(lazy_array)
+        
+        # Create dataset
+        ds = Dataset({
+            'temperature': (['x', 'y'], cached_array),
+            'x': ('x', np.arange(100)),
+            'y': ('y', np.arange(100))
+        })
+        
+        # Verify data is not in memory initially
+        temp_var = ds.variables['temperature']
+        assert not temp_var._in_memory
+        
+        # Access chunks property - this should not load data
+        chunks = ds.chunks
+        
+        # Verify data is still not in memory
+        assert not temp_var._in_memory
+        
+        # Verify chunks is empty (since this is not a chunked array)
+        assert chunks == {}
+
+    @requires_dask
+    def test_chunks_works_with_dask(self) -> None:
+        """Test that Dataset.chunks works correctly with dask arrays."""
+        import dask.array as da
+        
+        # Create dask arrays
+        temp_data = da.random.random((100, 100), chunks=(50, 50))
+        
+        # Create dataset
+        ds = Dataset({
+            'temperature': (['x', 'y'], temp_data),
+            'x': ('x', np.arange(100)),
+            'y': ('y', np.arange(100))
+        })
+        
+        # Verify data is not in memory initially
+        temp_var = ds.variables['temperature']
+        assert not temp_var._in_memory
+        
+        # Access chunks property
+        chunks = ds.chunks
+        
+        # Verify data is still not in memory
+        assert not temp_var._in_memory
+        
+        # Verify chunks are correct
+        expected_chunks = {'x': (50, 50), 'y': (50, 50)}
+        assert chunks == expected_chunks
+
     @requires_dask
     def test_dask_is_lazy(self) -> None:
         store = InaccessibleVariableDataStore()
diff --git a/xarray/tests/test_variable.py b/xarray/tests/test_variable.py
index 1fc34588..ba0529b3 100644
--- a/xarray/tests/test_variable.py
+++ b/xarray/tests/test_variable.py
@@ -13,7 +13,7 @@ from packaging.version import Version
 
 from xarray import Coordinate, DataArray, Dataset, IndexVariable, Variable, set_options
 from xarray.core import dtypes, duck_array_ops, indexing
-from xarray.core.common import full_like, ones_like, zeros_like
+from xarray.core.common import full_like, get_chunksizes, ones_like, zeros_like
 from xarray.core.indexing import (
     BasicIndexer,
     CopyOnWriteArray,
@@ -2749,4 +2749,52 @@ class TestNumpyCoercion:
 
         result = v.as_numpy()
         assert_identical(result, Var("x", arr))
-        np.testing.assert_equal(v.to_numpy(), arr)
+
+
+def test_get_chunksizes_does_not_load_data():
+    """Test that get_chunksizes doesn't trigger data loading for zarr-like arrays."""
+    # Create a numpy array that we'll wrap to simulate zarr backend behavior
+    base_array = np.random.random((100, 100))
+    
+    # Wrap it in adapters to simulate zarr backend
+    numpy_adapter = NumpyIndexingAdapter(base_array)
+    lazy_array = LazilyIndexedArray(numpy_adapter)
+    cached_array = MemoryCachedArray(lazy_array)
+    
+    # Create variable
+    var = Variable(['x', 'y'], cached_array)
+    
+    # Verify data is not in memory initially
+    assert not var._in_memory
+    
+    # Call get_chunksizes - this should not load data
+    chunks = get_chunksizes([var])
+    
+    # Verify data is still not in memory
+    assert not var._in_memory
+    
+    # Verify chunks is empty (since this is not a chunked array)
+    assert chunks == {}
+
+
+@requires_dask
+def test_get_chunksizes_works_with_dask():
+    """Test that get_chunksizes still works correctly with dask arrays."""
+    import dask.array as da
+    
+    # Create dask array
+    data = da.random.random((100, 100), chunks=(50, 50))
+    var = Variable(['x', 'y'], data)
+    
+    # Verify data is not in memory initially
+    assert not var._in_memory
+    
+    # Call get_chunksizes
+    chunks = get_chunksizes([var])
+    
+    # Verify data is still not in memory
+    assert not var._in_memory
+    
+    # Verify chunks are correct
+    expected_chunks = {'x': (50, 50), 'y': (50, 50)}
+    assert chunks == expected_chunks
