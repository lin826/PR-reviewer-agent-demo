diff --git a/examples/decomposition/plot_sparse_coding.py b/examples/decomposition/plot_sparse_coding.py
index 528817ad0..561d96272 100644
--- a/examples/decomposition/plot_sparse_coding.py
+++ b/examples/decomposition/plot_sparse_coding.py
@@ -102,4 +102,4 @@ for subplot, (D, title) in enumerate(zip((D_fixed, D_multi),
     plt.axis('tight')
     plt.legend(shadow=False, loc='best')
 plt.subplots_adjust(.04, .07, .97, .90, .09, .2)
-plt.show()
+plt.show()
diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index a318c957f..8462216a1 100644
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -865,7 +865,9 @@ class SparseCodingMixin(TransformerMixin):
                                   transform_algorithm='omp',
                                   transform_n_nonzero_coefs=None,
                                   transform_alpha=None, split_sign=False,
-                                  n_jobs=None, positive_code=False):
+
+                                  n_jobs=None, positive_code=False,
+                                  transform_max_iter=1000):
         self.n_components = n_components
         self.transform_algorithm = transform_algorithm
         self.transform_n_nonzero_coefs = transform_n_nonzero_coefs
@@ -873,6 +875,8 @@ class SparseCodingMixin(TransformerMixin):
         self.split_sign = split_sign
         self.n_jobs = n_jobs
         self.positive_code = positive_code
+        self.transform_max_iter = transform_max_iter
+
 
     def transform(self, X):
         """Encode the data as a sparse combination of the dictionary atoms.
@@ -896,12 +900,15 @@ class SparseCodingMixin(TransformerMixin):
 
         X = check_array(X)
 
+
         code = sparse_encode(
             X, self.components_, algorithm=self.transform_algorithm,
             n_nonzero_coefs=self.transform_n_nonzero_coefs,
             alpha=self.transform_alpha, n_jobs=self.n_jobs,
+            max_iter=self.transform_max_iter,
             positive=self.positive_code)
 
+
         if self.split_sign:
             # feature vector is split into a positive and negative side
             n_samples, n_features = code.shape
@@ -949,6 +956,7 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
         solution. This is only used by `algorithm='lars'` and `algorithm='omp'`
         and is overridden by `alpha` in the `omp` case.
 
+
     transform_alpha : float, 1. by default
         If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the
         penalty applied to the L1 norm.
@@ -958,11 +966,15 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
         the reconstruction error targeted. In this case, it overrides
         `n_nonzero_coefs`.
 
+    transform_max_iter : int, 1000 by default
+        Maximum number of iterations to perform if `algorithm='lasso_cd'`.
+
     split_sign : bool, False by default
         Whether to split the sparse feature vector into the concatenation of
         its negative part and its positive part. This can improve the
         performance of downstream classifiers.
 
+
     n_jobs : int or None, optional (default=None)
         Number of parallel jobs to run.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
@@ -989,16 +1001,29 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
     """
     _required_parameters = ["dictionary"]
 
+
+
     def __init__(self, dictionary, transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
-                 split_sign=False, n_jobs=None, positive_code=False):
+                 transform_max_iter=1000, max_iter=None, split_sign=False,
+                 n_jobs=None, positive_code=False):
+        # Handle the alias ``max_iter`` for backwardâ€‘compatibility.  Users are
+        # allowed to specify at most one of ``transform_max_iter`` or
+        # ``max_iter``.
+        if max_iter is not None:
+            if transform_max_iter != 1000 and transform_max_iter != max_iter:
+                raise ValueError("Specify at most one of 'transform_max_iter' "
+                                 "or 'max_iter'.")
+            transform_max_iter = max_iter
         self._set_sparse_coding_params(dictionary.shape[0],
                                        transform_algorithm,
                                        transform_n_nonzero_coefs,
                                        transform_alpha, split_sign, n_jobs,
-                                       positive_code)
+                                       positive_code, transform_max_iter)
+
         self.components_ = dictionary
 
+
     def fit(self, X, y=None):
         """Do nothing and return the estimator unchanged
 
@@ -1443,4 +1468,4 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):
         # some online fitting (partial_fit)
         self.inner_stats_ = (A, B)
         self.iter_offset_ = iter_offset + self.n_iter
-        return self
+        return self
diff --git a/sklearn/linear_model/coordinate_descent.py b/sklearn/linear_model/coordinate_descent.py
index 646839a0a..9a0451f91 100644
--- a/sklearn/linear_model/coordinate_descent.py
+++ b/sklearn/linear_model/coordinate_descent.py
@@ -2262,4 +2262,4 @@ class MultiTaskLassoCV(LinearModelCV, RegressorMixin):
             selection=selection)
 
     def _more_tags(self):
-        return {'multioutput_only': True}
+        return {'multioutput_only': True}
