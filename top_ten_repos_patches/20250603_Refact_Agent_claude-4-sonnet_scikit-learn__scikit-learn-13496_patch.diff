diff --git a/examples/ensemble/plot_isolation_forest.py b/examples/ensemble/plot_isolation_forest.py
index 1b79072df..ff700ebee 100644
--- a/examples/ensemble/plot_isolation_forest.py
+++ b/examples/ensemble/plot_isolation_forest.py
@@ -21,6 +21,10 @@ Random partitioning produces noticeable shorter paths for anomalies.
 Hence, when a forest of random trees collectively produce shorter path lengths
 for particular samples, they are highly likely to be anomalies.
 
+Note: IsolationForest supports incremental learning through the `warm_start`
+parameter. When `warm_start=True`, you can add more trees to an existing
+forest by increasing `n_estimators` and calling `fit()` again.
+
 """
 print(__doc__)
 
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index 8a1bd3625..f42c80532 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -120,6 +120,11 @@ class IsolationForest(BaseBagging, OutlierMixin):
     verbose : int, optional (default=0)
         Controls the verbosity of the tree building process.
 
+    warm_start : bool, optional (default=False)
+        When set to ``True``, reuse the solution of the previous call to fit
+        and add more estimators to the ensemble, otherwise, just fit a whole
+        new forest. See :term:`the Glossary <warm_start>`.
+
 
     Attributes
     ----------
@@ -173,7 +178,8 @@ class IsolationForest(BaseBagging, OutlierMixin):
                  n_jobs=None,
                  behaviour='old',
                  random_state=None,
-                 verbose=0):
+                 verbose=0,
+                 warm_start=False):
         super().__init__(
             base_estimator=ExtraTreeRegressor(
                 max_features=1,
@@ -187,7 +193,8 @@ class IsolationForest(BaseBagging, OutlierMixin):
             max_features=max_features,
             n_jobs=n_jobs,
             random_state=random_state,
-            verbose=verbose)
+            verbose=verbose,
+            warm_start=warm_start)
 
         self.behaviour = behaviour
         self.contamination = contamination
diff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py
index 67ba2d7f9..93c02c8ee 100644
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -359,3 +359,94 @@ def test_iforest_chunks_works2(
 ):
     test_iforest_works(contamination)
     assert mocked_get_chunk.call_count == n_predict_calls
+
+
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
+def test_iforest_warm_start():
+    """Test warm_start parameter in IsolationForest."""
+    # Test that warm_start parameter can be passed to constructor
+    clf = IsolationForest(n_estimators=5, warm_start=True, random_state=42)
+    assert clf.warm_start is True
+    
+    clf = IsolationForest(n_estimators=5, warm_start=False, random_state=42)
+    assert clf.warm_start is False
+    
+    # Test default value
+    clf = IsolationForest(n_estimators=5, random_state=42)
+    assert clf.warm_start is False
+
+
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
+def test_iforest_warm_start_functionality():
+    """Test that warm_start works correctly for incremental learning."""
+    X = iris.data
+    
+    # Test warm start behavior
+    clf = IsolationForest(n_estimators=5, warm_start=True, random_state=42)
+    clf.fit(X)
+    
+    # Check initial number of estimators
+    assert len(clf.estimators_) == 5
+    
+    # Increase n_estimators and fit again
+    clf.n_estimators = 10
+    clf.fit(X)
+    
+    # Should have 10 estimators now (5 old + 5 new)
+    assert len(clf.estimators_) == 10
+    
+    # Test that cold start retrains all estimators
+    clf_cold = IsolationForest(n_estimators=5, warm_start=False, random_state=42)
+    clf_cold.fit(X)
+    assert len(clf_cold.estimators_) == 5
+    
+    clf_cold.n_estimators = 10
+    clf_cold.fit(X)
+    assert len(clf_cold.estimators_) == 10
+
+
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
+def test_iforest_warm_start_predictions():
+    """Test that warm_start produces valid predictions."""
+    X = iris.data[:50]  # Use smaller dataset for faster testing
+    X_test = iris.data[50:55]
+    
+    # Create model with warm start
+    clf = IsolationForest(n_estimators=5, warm_start=True, random_state=42)
+    clf.fit(X)
+    
+    # Get predictions after first fit
+    pred1 = clf.predict(X_test)
+    
+    # Increase estimators and fit again
+    clf.n_estimators = 10
+    clf.fit(X)
+    
+    # Get predictions after warm start
+    pred2 = clf.predict(X_test)
+    
+    # Both predictions should be valid (same length as test data)
+    assert len(pred1) == len(X_test)
+    assert len(pred2) == len(X_test)
+    
+    # Predictions should be in {-1, 1}
+    assert_array_equal(np.unique(pred1), np.array([-1, 1]))
+    assert_array_equal(np.unique(pred2), np.array([-1, 1]))
+
+
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
+def test_iforest_warm_start_oob_error():
+    """Test that warm_start and oob_score are incompatible."""
+    # This should raise an error as warm_start and oob_score are incompatible
+    # in BaseBagging
+    with pytest.raises(ValueError, match="Out of bag estimate only available"):
+        clf = IsolationForest(warm_start=True, bootstrap=True)
+        # Note: IsolationForest doesn't support oob_score, but this tests
+        # the inherited behavior from BaseBagging
+        clf._set_oob_score = lambda X, y: None  # Mock to avoid NotImplementedError
+        clf.oob_score = True
+        clf.fit(iris.data)
