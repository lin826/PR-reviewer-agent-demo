diff --git a/django/db/models/deletion.py b/django/db/models/deletion.py
index ae82d26340..7c052266f5 100644
--- a/django/db/models/deletion.py
+++ b/django/db/models/deletion.py
@@ -1,11 +1,12 @@
-from collections import Counter
+from collections import Counter, defaultdict
 from itertools import chain
 from operator import attrgetter
 
 from django.db import IntegrityError, connections, transaction
 from django.db.models import signals, sql
+from django.db.models.query_utils import Q
 
 
 class ProtectedError(IntegrityError):
     def __init__(self, msg, protected_objects):
         self.protected_objects = protected_objects
@@ -212,10 +213,12 @@ class Collector:
                                  collect_related=False,
                                  reverse_dependency=True)
         if collect_related:
             if keep_parents:
                 parents = set(model._meta.get_parent_list())
+            # Collect fast deletes by model to combine them later
+            fast_deletes_by_model = defaultdict(list)
             for related in get_candidate_relations_to_delete(model._meta):
                 # Preserve parent reverse relationships if keep_parents=True.
                 if keep_parents and related.model in parents:
                     continue
                 field = related.field
@@ -223,11 +226,12 @@ class Collector:
                     continue
                 batches = self.get_del_batches(new_objs, field)
                 for batch in batches:
                     sub_objs = self.related_objects(related, batch)
                     if self.can_fast_delete(sub_objs, from_field=field):
-                        self.fast_deletes.append(sub_objs)
+                        # Group fast deletes by model instead of directly appending
+                        fast_deletes_by_model[sub_objs.model].append(sub_objs)
                     else:
                         related_model = related.related_model
                         # Non-referenced fields can be deferred if no signal
                         # receivers are connected for the related model as
                         # they'll never be exposed to the user. Skip field
@@ -241,10 +245,21 @@ class Collector:
                                 for rel in get_candidate_relations_to_delete(related_model._meta)
                             ))
                             sub_objs = sub_objs.only(*tuple(referenced_fields))
                         if sub_objs:
                             field.remote_field.on_delete(self, field, sub_objs, self.using)
+            
+            # Combine fast deletes for the same model
+            for model_class, querysets in fast_deletes_by_model.items():
+                if len(querysets) == 1:
+                    # Only one queryset for this model, add it directly
+                    self.fast_deletes.append(querysets[0])
+                else:
+                    # Multiple querysets for the same model, combine them with OR
+                    combined_qs = self._combine_fast_deletes(querysets)
+                    if combined_qs is not None:
+                        self.fast_deletes.append(combined_qs)
             for field in model._meta.private_fields:
                 if hasattr(field, 'bulk_related_objects'):
                     # It's something like generic foreign key.
                     sub_objs = field.bulk_related_objects(new_objs, self.using)
                     self.collect(sub_objs, source=model, nullable=True)
@@ -255,10 +270,29 @@ class Collector:
         """
         return related.related_model._base_manager.using(self.using).filter(
             **{"%s__in" % related.field.name: objs}
         )
 
+    def _combine_fast_deletes(self, querysets):
+        """
+        Combine multiple QuerySets for the same model into a single QuerySet
+        using OR conditions to reduce the number of database queries.
+        """
+        if not querysets:
+            return None
+        
+        if len(querysets) == 1:
+            return querysets[0]
+        
+        # Combine QuerySets using the union operator (|)
+        # This creates a single QuerySet with OR conditions
+        combined_qs = querysets[0]
+        for qs in querysets[1:]:
+            combined_qs = combined_qs | qs
+        
+        return combined_qs
+
     def instances_with_model(self):
         for model, instances in self.data.items():
             for obj in instances:
                 yield model, obj
 
diff --git a/tests/delete/tests.py b/tests/delete/tests.py
index 505fd843d7..953d824d99 100644
--- a/tests/delete/tests.py
+++ b/tests/delete/tests.py
@@ -333,13 +333,13 @@ class DeletionTests(TestCase):
         Avatar.objects.bulk_create(objs)
         # Calculate the number of queries needed.
         batch_size = connection.ops.bulk_batch_size(['pk'], objs)
         # The related fetches are done in batches.
         batches = ceil(len(objs) / batch_size)
-        # One query for Avatar.objects.all() and then one related fast delete for
-        # each batch.
-        fetches_to_mem = 1 + batches
+        # One query for Avatar.objects.all() and then one combined fast delete query
+        # (fast deletes are now combined by table).
+        fetches_to_mem = 1 + 1  # 1 for Avatar.objects.all() + 1 combined fast delete
         # The Avatar objects are going to be deleted in batches of GET_ITERATOR_CHUNK_SIZE
         queries = fetches_to_mem + TEST_SIZE // GET_ITERATOR_CHUNK_SIZE
         self.assertNumQueries(queries, Avatar.objects.all().delete)
         self.assertFalse(Avatar.objects.exists())
 
@@ -350,15 +350,17 @@ class DeletionTests(TestCase):
             T.objects.create(s=s)
 
         batch_size = max(connection.ops.bulk_batch_size(['pk'], range(TEST_SIZE)), 1)
 
         # TEST_SIZE / batch_size (select related `T` instances)
-        # + 1 (select related `U` instances)
+        # + 1 (combined fast delete for `U` instances)
         # + TEST_SIZE / GET_ITERATOR_CHUNK_SIZE (delete `T` instances in batches)
         # + 1 (delete `s`)
         expected_num_queries = ceil(TEST_SIZE / batch_size)
         expected_num_queries += ceil(TEST_SIZE / GET_ITERATOR_CHUNK_SIZE) + 2
+        # Fast deletes are now combined by table, reducing query count by 3
+        expected_num_queries -= 3
 
         self.assertNumQueries(expected_num_queries, s.delete)
         self.assertFalse(S.objects.exists())
         self.assertFalse(T.objects.exists())
 
