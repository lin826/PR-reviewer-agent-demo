diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index 911c74ae3..6469cdbcf 100644
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -1301,6 +1301,12 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
         weights inversely proportional to class frequencies in the input data
         as ``n_samples / (n_classes * np.bincount(y))``
 
+    store_cv_values : boolean, default=False
+        Flag indicating if the cross-validation values corresponding to
+        each alpha should be stored in the `cv_values_` attribute (see
+        below). This flag is only compatible with `cv=None` (i.e. using
+        Generalized Cross-Validation).
+
     Attributes
     ----------
     cv_values_ : array, shape = [n_samples, n_alphas] or \
@@ -1333,10 +1339,11 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
     advantage of the multi-variate response support in Ridge.
     """
     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,
-                 normalize=False, scoring=None, cv=None, class_weight=None):
+                 normalize=False, scoring=None, cv=None, class_weight=None,
+                 store_cv_values=False):
         super(RidgeClassifierCV, self).__init__(
             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,
-            scoring=scoring, cv=cv)
+            scoring=scoring, cv=cv, store_cv_values=store_cv_values)
         self.class_weight = class_weight
 
     def fit(self, X, y, sample_weight=None):
diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py
index ee44da5d5..e6d48860e 100644
--- a/sklearn/linear_model/tests/test_ridge.py
+++ b/sklearn/linear_model/tests/test_ridge.py
@@ -35,6 +35,7 @@ from sklearn.model_selection import KFold
 
 from sklearn.utils import check_random_state
 from sklearn.datasets import make_multilabel_classification
+from sklearn.datasets import make_classification
 
 diabetes = datasets.load_diabetes()
 X_diabetes, y_diabetes = diabetes.data, diabetes.target
@@ -860,3 +861,49 @@ def test_dtype_match_cholesky():
     assert ridge_32.predict(X_32).dtype == X_32.dtype
     assert ridge_64.predict(X_64).dtype == X_64.dtype
     assert_almost_equal(ridge_32.coef_, ridge_64.coef_, decimal=5)
+
+
+def test_ridgeclassifiercv_store_cv_values():
+    """Test that RidgeClassifierCV supports store_cv_values parameter."""
+    X, y = make_classification(n_samples=50, n_features=10, n_classes=2, 
+                              random_state=42)
+    
+    # Test with store_cv_values=True
+    clf = RidgeClassifierCV(alphas=[0.1, 1.0, 10.0], store_cv_values=True)
+    clf.fit(X, y)
+    
+    # Should have cv_values_ attribute
+    assert_true(hasattr(clf, 'cv_values_'))
+    assert_true(clf.cv_values_ is not None)
+    assert_equal(clf.cv_values_.shape, (X.shape[0], 1, len(clf.alphas)))
+    
+    # Test with store_cv_values=False (default)
+    clf_default = RidgeClassifierCV(alphas=[0.1, 1.0, 10.0])
+    clf_default.fit(X, y)
+    
+    # Should not have cv_values_ attribute
+    assert_true(not hasattr(clf_default, 'cv_values_'))
+
+
+def test_ridgeclassifiercv_store_cv_values_multiclass():
+    """Test store_cv_values with multiclass classification."""
+    X, y = make_classification(n_samples=50, n_features=10, n_classes=3, 
+                              n_informative=8, random_state=42)
+    
+    clf = RidgeClassifierCV(alphas=[0.1, 1.0, 10.0], store_cv_values=True)
+    clf.fit(X, y)
+    
+    # Should have cv_values_ attribute with correct shape for multiclass
+    assert_true(hasattr(clf, 'cv_values_'))
+    assert_equal(clf.cv_values_.shape, (X.shape[0], 3, len(clf.alphas)))
+
+
+def test_ridgeclassifiercv_store_cv_values_incompatible_with_cv():
+    """Test that store_cv_values=True with cv!=None raises ValueError."""
+    X, y = make_classification(n_samples=50, n_features=10, n_classes=2, 
+                              random_state=42)
+    
+    clf = RidgeClassifierCV(alphas=[0.1, 1.0, 10.0], cv=5, store_cv_values=True)
+    
+    assert_raise_message(ValueError, "cv!=None and store_cv_values=True",
+                        clf.fit, X, y)
