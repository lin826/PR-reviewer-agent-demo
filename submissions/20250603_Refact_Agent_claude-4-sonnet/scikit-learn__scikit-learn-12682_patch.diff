diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index a318c957f..bff1718cb 100644
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -32,7 +32,8 @@ def _check_positive_coding(method, positive):
 def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
                    regularization=None, copy_cov=True,
                    init=None, max_iter=1000, check_input=True, verbose=0,
-                   positive=False):
+                   positive=False, tol=1e-4, selection='cyclic',
+                   random_state=None):
     """Generic sparse coding
 
     Each column of the result is the solution to a Lasso problem.
@@ -141,7 +142,8 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
         # argument that we could pass in from Lasso.
         clf = Lasso(alpha=alpha, fit_intercept=False, normalize=False,
                     precompute=gram, max_iter=max_iter, warm_start=True,
-                    positive=positive)
+                    positive=positive, tol=tol, selection=selection,
+                    random_state=random_state)
 
         if init is not None:
             clf.coef_ = init
@@ -187,7 +189,8 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',
 def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
                   n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None,
                   max_iter=1000, n_jobs=None, check_input=True, verbose=0,
-                  positive=False):
+                  positive=False, tol=1e-4, selection='cyclic',
+                  random_state=None):
     """Sparse coding
 
     Each row of the result is the solution to a sparse coding problem.
@@ -265,6 +268,33 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
 
         .. versionadded:: 0.20
 
+    tol : float, default=1e-4
+        The tolerance for the optimization: if the updates are
+        smaller than `tol`, the optimization code checks the
+        dual gap for optimality and continues until it is smaller
+        than `tol`. Only used if `algorithm='lasso_cd'`.
+
+        .. versionadded:: 0.22
+
+    selection : str, default='cyclic'
+        If set to 'random', a random coefficient is updated every iteration
+        rather than looping over features sequentially by default. This
+        (setting to 'random') often leads to significantly faster convergence
+        especially when tol is higher than 1e-4. Only used if 
+        `algorithm='lasso_cd'`.
+
+        .. versionadded:: 0.22
+
+    random_state : int, RandomState instance or None, default=None
+        The seed of the pseudo random number generator that selects a random
+        feature to update. If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when 
+        `selection='random'` and `algorithm='lasso_cd'`.
+
+        .. versionadded:: 0.22
+
     Returns
     -------
     code : array of shape (n_samples, n_components)
@@ -313,7 +343,9 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
                               max_iter=max_iter,
                               check_input=False,
                               verbose=verbose,
-                              positive=positive)
+                              positive=positive,
+                              tol=tol, selection=selection,
+                              random_state=random_state)
         return code
 
     # Enter parallel code block
@@ -329,7 +361,9 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',
             init=init[this_slice] if init is not None else None,
             max_iter=max_iter,
             check_input=False,
-            positive=positive)
+            positive=positive,
+            tol=tol, selection=selection,
+            random_state=random_state)
         for this_slice in slices)
     for this_slice, this_view in zip(slices, code_views):
         code[this_slice] = this_view
@@ -865,7 +899,10 @@ class SparseCodingMixin(TransformerMixin):
                                   transform_algorithm='omp',
                                   transform_n_nonzero_coefs=None,
                                   transform_alpha=None, split_sign=False,
-                                  n_jobs=None, positive_code=False):
+                                  n_jobs=None, positive_code=False,
+                                  transform_max_iter=1000, transform_tol=1e-4,
+                                  transform_selection='cyclic',
+                                  transform_random_state=None):
         self.n_components = n_components
         self.transform_algorithm = transform_algorithm
         self.transform_n_nonzero_coefs = transform_n_nonzero_coefs
@@ -873,6 +910,10 @@ class SparseCodingMixin(TransformerMixin):
         self.split_sign = split_sign
         self.n_jobs = n_jobs
         self.positive_code = positive_code
+        self.transform_max_iter = transform_max_iter
+        self.transform_tol = transform_tol
+        self.transform_selection = transform_selection
+        self.transform_random_state = transform_random_state
 
     def transform(self, X):
         """Encode the data as a sparse combination of the dictionary atoms.
@@ -900,7 +941,9 @@ class SparseCodingMixin(TransformerMixin):
             X, self.components_, algorithm=self.transform_algorithm,
             n_nonzero_coefs=self.transform_n_nonzero_coefs,
             alpha=self.transform_alpha, n_jobs=self.n_jobs,
-            positive=self.positive_code)
+            positive=self.positive_code, max_iter=self.transform_max_iter,
+            tol=self.transform_tol, selection=self.transform_selection,
+            random_state=self.transform_random_state)
 
         if self.split_sign:
             # feature vector is split into a positive and negative side
@@ -974,6 +1017,38 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
 
         .. versionadded:: 0.20
 
+    transform_max_iter : int, default=1000
+        Maximum number of iterations to perform if `transform_algorithm='lasso_cd'`.
+
+        .. versionadded:: 0.22
+
+    transform_tol : float, default=1e-4
+        The tolerance for the optimization: if the updates are
+        smaller than `transform_tol`, the optimization code checks the
+        dual gap for optimality and continues until it is smaller
+        than `transform_tol`. Only used if `transform_algorithm='lasso_cd'`.
+
+        .. versionadded:: 0.22
+
+    transform_selection : str, default='cyclic'
+        If set to 'random', a random coefficient is updated every iteration
+        rather than looping over features sequentially by default. This
+        (setting to 'random') often leads to significantly faster convergence
+        especially when tol is higher than 1e-4. Only used if 
+        `transform_algorithm='lasso_cd'`.
+
+        .. versionadded:: 0.22
+
+    transform_random_state : int, RandomState instance or None, default=None
+        The seed of the pseudo random number generator that selects a random
+        feature to update. If int, random_state is the seed used by the random
+        number generator; If RandomState instance, random_state is the random
+        number generator; If None, the random number generator is the
+        RandomState instance used by `np.random`. Used when 
+        `transform_selection='random'` and `transform_algorithm='lasso_cd'`.
+
+        .. versionadded:: 0.22
+
     Attributes
     ----------
     components_ : array, [n_components, n_features]
@@ -991,12 +1066,16 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
 
     def __init__(self, dictionary, transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
-                 split_sign=False, n_jobs=None, positive_code=False):
+                 split_sign=False, n_jobs=None, positive_code=False,
+                 transform_max_iter=1000, transform_tol=1e-4,
+                 transform_selection='cyclic', transform_random_state=None):
         self._set_sparse_coding_params(dictionary.shape[0],
                                        transform_algorithm,
                                        transform_n_nonzero_coefs,
                                        transform_alpha, split_sign, n_jobs,
-                                       positive_code)
+                                       positive_code, transform_max_iter,
+                                       transform_tol, transform_selection,
+                                       transform_random_state)
         self.components_ = dictionary
 
     def fit(self, X, y=None):
diff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py
index f0bd4bedf..8b084b538 100644
--- a/sklearn/decomposition/tests/test_dict_learning.py
+++ b/sklearn/decomposition/tests/test_dict_learning.py
@@ -1,4 +1,5 @@
 import pytest
+import warnings
 
 import numpy as np
 import itertools
@@ -451,3 +452,71 @@ def test_sparse_coder_parallel_mmap():
 
     sc = SparseCoder(init_dict, transform_algorithm='omp', n_jobs=2)
     sc.fit_transform(data)
+
+
+def test_sparse_coder_max_iter():
+    """Test that SparseCoder exposes max_iter parameter for lasso_cd algorithm."""
+    # Create a simple test case
+    rng = np.random.RandomState(42)
+    n_components, n_features = 10, 20
+    dictionary = rng.randn(n_components, n_features)
+    dictionary = dictionary / np.sqrt(np.sum(dictionary ** 2, axis=1))[:, np.newaxis]
+    
+    X = rng.randn(5, n_features)
+    
+    # Test 1: Default max_iter should work
+    coder = SparseCoder(dictionary=dictionary, transform_algorithm='lasso_cd',
+                       transform_alpha=0.1)
+    code = coder.transform(X)
+    assert_equal(code.shape, (5, n_components))
+    
+    # Test 2: Custom max_iter should work
+    coder = SparseCoder(dictionary=dictionary, transform_algorithm='lasso_cd',
+                       transform_alpha=0.1, transform_max_iter=2000)
+    code = coder.transform(X)
+    assert_equal(code.shape, (5, n_components))
+    
+    # Test 3: Other new parameters should work
+    coder = SparseCoder(dictionary=dictionary, transform_algorithm='lasso_cd',
+                       transform_alpha=0.1, transform_max_iter=2000,
+                       transform_tol=1e-3, transform_selection='random',
+                       transform_random_state=42)
+    code = coder.transform(X)
+    assert_equal(code.shape, (5, n_components))
+    
+    # Test 4: Backward compatibility - old parameters should still work
+    coder = SparseCoder(dictionary=dictionary, transform_algorithm='omp',
+                       transform_n_nonzero_coefs=5)
+    code = coder.transform(X)
+    assert_equal(code.shape, (5, n_components))
+
+
+def test_sparse_coder_max_iter_convergence():
+    """Test that higher max_iter can reduce convergence warnings."""
+    # Create a challenging optimization problem that may not converge quickly
+    rng = np.random.RandomState(42)
+    n_components, n_features = 50, 100
+    dictionary = rng.randn(n_components, n_features)
+    dictionary = dictionary / np.sqrt(np.sum(dictionary ** 2, axis=1))[:, np.newaxis]
+    
+    # Create a signal that's hard to fit
+    X = rng.randn(1, n_features) * 10
+    
+    # Test with low max_iter (likely to show warning)
+    with warnings.catch_warnings(record=True) as w:
+        warnings.simplefilter("always")
+        coder = SparseCoder(dictionary=dictionary, transform_algorithm='lasso_cd',
+                           transform_alpha=1.0, transform_max_iter=10)
+        code = coder.transform(X)
+        low_iter_warnings = len(w)
+    
+    # Test with high max_iter (should reduce warnings)
+    with warnings.catch_warnings(record=True) as w:
+        warnings.simplefilter("always")
+        coder = SparseCoder(dictionary=dictionary, transform_algorithm='lasso_cd',
+                           transform_alpha=1.0, transform_max_iter=5000)
+        code = coder.transform(X)
+        high_iter_warnings = len(w)
+    
+    # Higher max_iter should result in fewer or equal warnings
+    assert high_iter_warnings <= low_iter_warnings
